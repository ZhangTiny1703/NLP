循环神经网络RNN特点：
  记忆特性
  接收两个参数（输入x，和上一时间的输出o）
  参数共享（权重w 确保每一步在做相同的事）
  按照时间序列进行：W=W0*W1*W2*...Wn,W<1,即梯度消失;W>1,即梯度爆炸

循环神经网络RNN的网络结构：
  one-to-one:分类
    x->O->o
    
  one-to-many:图片描述，音乐生成（多序列）
    x->O->o
     ->O->o
     ->O->o
     
  many-to-one:句子输入，多分类
    x->O
    x->O
    x->O->o
    
  many-to-many(1):语言翻译
    x->O
    x->O
    x->O->o
       O->o
       O->o
       
  many-to-many(2):命名实体识别
    x->O->o
    x->O->o
    x->O->o

双向循环神经网络:
  每个时刻有两个隐藏层
  一个从左到右，一个从右到左
  向前和向后传播参数独立
 
解决梯度消失和梯度爆炸的方法：
  选择合适的激活参数
    一般使用ReLu函数，导数为1
    不建议使用sigmoid和Tanh函数，区域小，会导致梯度消散
    
  选择合适的参数初始化方法
    参数的大小，会影响梯度下降的效果
    不可以把所有参数都初始化为0
    可以使用W[L]=np.random.randn(shape[L])*0.01 (W[L]是第L层的权重参数，shape是第L层权重参数矩阵的形状) 有可能会导致梯度消散
    可以使用W[L]=np.random.randn(shape[L])*np.sqrt(1/n[L-1]) (W[L]是第L层的权重参数，shape是第L层权重参数矩阵的形状,n[L-1]是L-1层的神经元数)  缓解梯度消散的速度
  
  使用权重参数正则化
  
  使用BatchNormalization
    通过规范化操作将输出信号x规范化到均值为0，方差为1,保证网络的稳定性
    可以加大神经网络训练的速度
    提高训练的稳定性
    缓解梯度爆炸和梯度消散的问题
    
  使用残差结构（最有效，最重要的解决方法）
    提高的神经网络的深度
    
  使用梯度裁剪
    if ||g||>v
    g <- gv / ||g||
    其中v是梯度范数的上界，g用来更新参数的梯度
